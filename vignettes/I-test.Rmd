---
title: "The $I$-test"
author:
- name: Orestis Loukas
  affiliation: Philipps-University Marburg, Germany
- name: Ho-Ryun Chung
  affiliation: Philipps-University Marburg, Germany
  email: ho.chung@staff.uni-marburg.de
package: totem
output:
  BiocStyle::html_document
abstract: |
  Statistical testing is a common procedure to demarcate "good" from "bad" theories. Here,    we provide a reference implementation for our totally empirical (TOTEM) statistics          framework. We implement the two-way $I$-test for testing group mean and rate differences    as well as arbitrary relationships between two metric variables. 
  
  totem package version: `r packageVersion("totem")` 
vignette: |
  %\VignetteIndexEntry{totem}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
# Installing TOTEM
```{r installTOTEM, eval=FALSE}
if (! requireNamespace("devtools", quietly = TRUE))
  install.packages("devtools")
  
devtools::install_github("horyunchung/totem", dependencies = TRUE, build_vignettes = TRUE, )
```

# Using the $I$-test
Load the `totem` package
```{r setup}
library(totem)
```
Currently only two-way tests for dichotomous categorical response against a dichotomous categorical attribute, metric response against a dichotomous categorical attribute, and metric response against metric attribute is implemented.

The interface uses a formula of the form `lhs ~ rhs`, where `lhs` corresponds to the "response", which is either a dichotomous (binary) character/factor or a numeric variable and `rhs` corresponds to the "predictor", which is either a dichotomous (binary) character/factor (for categorical or metric response) or a numeric variable (for metric response only).

## One-sample $I$-test

### Dichotomous categorical response
This corresponds to an exact binomial test. We use the example provided in the help file for the `binom.test` function: "Under (the assumption of) simple Mendelian inheritance, a cross between plants of two particular genotypes produces progeny 1/4 of which are "dwarf" and 3/4 of which are "giant", respectively. In an experiment to determine if this assumption is reasonable, a cross results in progeny having 243 dwarf and 682 giant plants."

```{r phenotype}
phenotype <- factor(
  c(rep("giant", 682), rep("dwarf", 243)), 
  levels = c("dwarf", "giant")
)
```

If "giant" is taken as success, the null hypothesis is that the probability for "giant" (success) is
$$
  H_0: p = 3/4
$$
and the alternative is
$$
  H_1: p \ne 3/4
$$
We test this null hypothesis (at a significance level of 5%) using the exact binomial test
```{r binom.test}
binom.test(table(phenotype)[c("giant", "dwarf")], p = 3/4)
```
and using the $I$-test
```{r oneSample.i.test.rates}
i.test(phenotype ~ 1, data = data.frame(phenotype = phenotype), mu = 3/4)
```
Note that the 2nd level of the categorical variable on the `lhs` is taken as "success".


### Metric response
This corresponds to a one-sample $t$-test. We use the classical example provided in the help file for the `t.test` function: "Student's sleep data".

The null hypothesis is that the mean increase in hours of sleep is zero.
$$
  H_0: \mu_\text{extra} = 0
$$
and the alternative is
$$
  H_1: \mu_\text{extra} \ne 0
$$
We test this null hypothesis (at a significance level of 5%) using the one-sample $t$-test:
```{r oneSample.t.test}
t.test(extra ~ 1, data = sleep)
```
and using the $I$-test
```{r oneSample.i.test.means}
i.test(extra ~ 1, data = sleep, mu = 0)
```

## Two-sample $I$-test

### Dichotomous categorical response against a dichotomous categorical attribute
This corresponds to Fisher's Exact test, Bernard's test, or Pearson's $\chi^2$ test. We take the example provided in the help file for the `fisher.test` function:
```{r convictionData}
convictions <- data.frame(
  twinType = c(rep("Dizygotic", 17), rep("Monozygotic", 13)),
  conviction = c(rep("yes", 2), rep("no", 15), rep("yes", 10), rep("no", 3))
)
```
With corresponding 2-way contingency table
```{r contingencyTable}
table(convictions)
```

The null hypothesis for Fisher's exact test is:
$$
  H_0: \frac{\text{Odds}(\text{conviction = yes}|\text{twinType = Dizygotic})}{\text{Odds}(\text{conviction = yes}|\text{twinType = Monozygotic})} = 1
$$
and the alternative is:
$$
  H_1: \frac{\text{Odds}(\text{conviction = yes}|\text{twinType = Dizygotic})}{\text{Odds}(\text{conviction = yes}|\text{twinType = Monozygotic})} \ne 1
$$
We test this null hypothesis (at a significance level of 5%) using Fisher's exact test:
```{r fisher.test}
fisher.test(table(convictions)[, c("yes", "no")])
```
We have not implemented the "tea-tasting" study design, where both marginal probabilities for the twinType attribute as well as the conviction attribute are fixed. This study design leads to four conditions for the four (unknown) probabilities of the hypothesis distribution.

1. the conditional rate for conviction for mono- and dizygotic twins is equal
2. the overall conviction rate is fixed to the observed one
3. the overal rate for monozygotic twins is fixed to the observed one
4. all probabilities sum up to one

Which means that the hypothesis distribution is given by
$$
  p(Y, X) = f_Y(Y)~f_X(X)
$$

```{r}
## convert to entities
entities <- toEntities((convictions[, c("conviction", "twinType")]))
## marginals
marginalTwinType = with(entities, tapply(empirical, twinType, sum))
marginalConviction = with(entities, tapply(empirical, conviction, sum))
## hypothesis distribution
(pH <- with(
  entities, 
  marginalTwinType[twinType] * marginalConviction[conviction]
))
```
All we have to do is to calculate the $I$-divergence of the hypothesis distribution to the empirical distribution 
```{r}
(idiv <- iDivergence(entities$empirical, pH))
```
which gives a $p$-value according to the $\chi^2$ distribution with 1 degree of freedom for the statistic $t = 2~N~D(\mathfrak{f}\vert\vert \mathfrak{p}_\text{H})$
```{r}
pchisq(2 * attr(entities, "N") * idiv, df = 1, lower.tail = FALSE)
```

Note that Fisher's exact test assumes that both the marginal probabilities for the twinType attribute as well as the conviction attribute are fixed (prespecifiable!). This is not the case. So let's assume that only the marginal probabilities for the twinType attribute had been fixed in advance ("experimental study design"). In this scenario we should use Barnard's test instead:
```{r}
library(Barnard)
barnard.test(2,10,15,3)
```
Alternatively, we could use the $I$-test for an experimental study design as outline above by setting the option `fix` to `TRUE`:
```{r}
i.test(conviction ~ twinType, data = convictions, fix = TRUE)
```
Finally, if we cannot assume that the probability of the twinType was fixed by the study design, we need to resort to the "observational study design": In this scenario we should use Pearson's $\chi^2$ test:
```{r}
prop.test(table(convictions)[, c("yes", "no")])
```
or with the $I$-test for an observational study design as outline above by setting the option `fix` to `FALSE`:
```{r}
i.test(conviction ~ twinType, data = convictions, fix = FALSE)
```

### Metric response against a dichotomous categorical attribute
This is the classical two-sample $t$-test scenario. We take the Student's sleep data used for the one-sample example.
The null hypothesis is that the average increase in hours of sleep is the same for the two drugs given:
$$
  H_0: \mu_\text{drug 1} = \mu_\text{drug 2}
$$
and the alternative is:
$$
  H_1: \mu_\text{drug 1} \ne \mu_\text{drug 2}
$$
We test the null hypothesis at a signficance level of 5% using the Student's $t$-test
```{r twoSampleStudent}
t.test(extra ~ group, data = sleep, var.equal = TRUE)
```
or using Welch's $t$-test
```{r twoSampleWelch}
t.test(extra ~ group, data = sleep, var.equal = FALSE)
```
or using the $I$-test in the experimental setting (group sizes are fixed):
```{r twoSample.i.t.test.MetricExperimental}
i.test(extra ~ group, data = sleep, fix = TRUE)
```
or using the $I$-test in the observational setting:
```{r twoSample.i.test.MetricObservational}
i.test(extra ~ group, data = sleep, fix = FALSE)
```
In contrast to the $t$-test the $I$-test detects a significant difference! Moreover, the $t$-test cannot account for the different study designs, which in this example lead to only a small difference.

Actually the sleep data is paired, i.e. the study design is a cross-over study, where the same patient takes drug 1 and drug 2.

So we can formulate a much better informed null hypothesis, namely that the mean difference in the increase in hours sleep of drug 2 versus drug 1 (per participant) is zero.
We reformat the sleep data.frame from long to wide format
```{r, longToWide}
sleep2 <- reshape(sleep, direction = "wide",
                  idvar = "ID", timevar = "group")
```
We will evaluate the null hypothesis that the mean difference of the increase in hours sleep for drug 2 versus drug 1 is zero:
$$
  H_0: E(Y_\text{drug 2} - Y_\text{drug 1}) = 0 
$$
and the alternative
$$
  H_1: E(Y_\text{drug 2} - Y_\text{drug 1}) \ne 0 
$$
The $t$-test yields this result:
```{r, paired.t.test}
t.test(Pair(extra.2, extra.1) ~ 1, data = sleep2)
```
While the $I$-test this one:
```{r, paired.i.test}
i.test(I(extra.2 - extra.1) ~ 1, data = sleep2)
```
Here, the $I$-test does not report a result. This can be understood upon looking at the values of the difference:
```{r}
with(sleep2, stripchart(extra.2 - extra.1), pch = 16)
```
All differences are equal to or larger than zero. The only way to obtain the hypothesis distribution with a mean difference of zero is to put all weight to the participant with zero difference. The `NA` for the $p$-value usually signals that the hypothesis TOTEMplex $\mathcal{T}_\text{H}$ is empty, i.e. formally no $p$-value can be calculated. However, here the generic implementation of the $I$-test breaks, because the hypothesis condition that the mean difference is zero becomes redundant to the structural normalization condition. Thus, the hypothesis TOTEMplex $\mathcal{T}_\text{H}$ has only one member (the one that puts all weight to the patient with zero difference).  But from this hypothesis distribution, it is impossible to sample back datasets with an observed mean difference of $\mu = `r with(sleep2, mean(extra.2 - extra.1))`$. Thus, the empirical $p$-value is zero.

### Metric response against a metric attribute
Finally, we want to test two so-called allometric laws that relate the basal metabolic rate (BMR) to the body weight. The first law is referred to as Rubner's law and has the following form:
$$
  \langle Y \rangle = \left\langle \frac{Y}{X^{2/3}} \right\rangle~\langle X^{2/3}\rangle.
$$
This is a "mild" condition on the law, i.e. we do not expect that all species obey Rubner's law (fundamental law) but that they obey it in expectation (effective law).

The second law is referred to as Kleiber's law:
$$
  \langle Y \rangle = \left\langle \frac{Y}{X^{3/4}} \right\rangle~\langle X^{3/4}\rangle.
$$

We will be using data from Genoud et al. (2018) for the basal metabolic rate and body mass of 549 mammalian species:
```{r bmrMassData}
data("bmrMass")
```
We can test the hypothesis that Rubner's law is obeyed in expectation by using the $I$-test
```{r}
i.test(BMR ~ I(Mass^(2/3)), data = bmrMass)
```
Per default the $I$-test will estimate the fractional moment $\langle X^{2/3}\rangle$, i.e. the study design is per default set to observational. Here, the test result shows that we can construct the hypothesis distribution. However, we cannot sample any dataset from it that has a bias at least as large as the observed one! Hence, the $p$-value is zero.

If we knew the fractional moment $\langle X^{2/3}\rangle$ beforehand, we could prespecify it and use the $I$-test for an experimental study design:
```{r}
i.test(BMR ~ I(Mass^(2/3)), data = bmrMass, fix = TRUE)
```
So for both study designs, we can reject the null hypothesis, i.e. Rubner's law is refuted by the data.

For Kleiber's law, we use the $I$-test for the observational study design
```{r}
i.test(BMR ~ I(Mass^(3/4)), data = bmrMass)
```
or we could use the experimental study design
```{r}
i.test(BMR ~ I(Mass^(3/4)), data = bmrMass, fix = TRUE)
```
Note the one-order-of-magnitude difference between the $p$-value for the observational versus experimental study design. This underlines again that in order to enjoy the increased statistical power obtained by controlling the predictor attribute this can and should be prespecified to prevent $p$-value hacking.

Both $I$-tests reveal a $p$-value smaller than the 5% significance level, i.e. also Kleiber's law is refuted by the data!

## Generalized $I$-test
We have implemented some standard testing scenarios. However, the $I$-test is much more versatile. 

### Equality of means for more than two groups
For this example we use the `iris` dataset. Here, we want to know whether the attribute ``r (what = "Sepal.Length")`` differs between the three species of irs. The standard approach of the "old statistics" is the one-factorial ANOVA. The validity of the results of an one-factorial ANOVA depends on the validity of two assumptions:

1. **Normality**: The ``r what`` attribute should follow a normal distribution in each species.  
   We check this assumption by applying the Shapiro-Wilks test for normality for the ``r what`` attribute in each species
   ```{r}
   tapply(iris[, what], iris$Species, shapiro.test)
   ```  
   Normality cannot be rejected (at the 5% significance level). 

2. **Homoscedasticity**: The variance of the ``r what`` attribute should be equal
   We check this assumption by applying Bartlett's test of the homogeneity of variances:
   ```{r}
   bartlett.test(formula(paste(what, "~Species")), data = iris)
   ```
   Here, the null hypothesis that the variances are equal must be rejected.

Formally speaking this invalidates the results of an one-factorial ANOVA, such that it should not be used. The one-factorial ANOVA gives following result
```{r oneFactorialANOVA}
summary(aov(as.formula(paste(what, "~ Species")), data = iris))
```
The $I$-test does not require these assumptions. So we implement here (the application-dependent) $I$-test for this test scenario:

1. Convert the data into the entity representation
   ```{r toEntities}
   entities <- toEntities(subset(iris, select = c(what, "Species")))
   ```

2. Compute the marginal probabilities of the species
   ```{r marginalSpecies}
   (marginalSpecies <- with(entities, tapply(empirical, Species, sum)))
   ```
3. Define the coefficient matrix $\mathbf{G}$
   ```{r coefficientMatrix}
   G <- rbind(
      ## hypothesis constraints
      ifelse(
        entities$Species == "versicolor", 
        entities[, what] / marginalSpecies["versicolor"], 
        ifelse(entities$Species == "setosa", 
           -entities[, what] / marginalSpecies["setosa"], 
          0
        )
      ),
      ifelse(
        entities$Species == "virginica", 
          entities[, what] / marginalSpecies["virginica"], 
          ifelse(entities$Species == "setosa", 
            -entities[, what] / marginalSpecies["setosa"], 
            0
          )
      ),
      ## structural constraints
      ifelse(
        entities$Species == "setosa",
        1,
        0
      ),
      ifelse(
        entities$Species == "versicolor", 
        1,
        0
      ),
      ifelse(
        entities$Species == "virginica", 
        1,
        0
      )
   )
   ```
  The first row of $\mathbf{G}$ computes the difference of the conditional mean of the ``r what`` attribute of the species "versicolor" minus the one of "setosa". The second row computes the difference of the conditional mean of the ``r what`` attribute of the species "virginica" minus the one of "setosa". Our null hypothesis conditions these two differences to be zero. The following three rows compute the marginal probabilities of the three species (which, since they sum to one, imply the normalization condition).

4. Define the conditions for the null hypothesis
   ```{r mu}
   mu <- c(0,0, marginalSpecies)
   ```
5. Compute the $I$-projection of the empirical distribution $\mathfrak{f}$ (stored in the `empirical` column of the `entities` data.frame) onto the hypothesis TOTEMplex $\mathcal{T}_\text{H}$:
   ```{r hypothesisDistribution}
   pH <- iProjector(G, targets = c(0,0, marginalSpecies), v = entities$empirical)
   ```
6. Compute the alternative distribution $\mathfrak{p}_\text{A}$ as the $I$-projection of the hypothesis distribution to the alternative TOTEMplex $\mathcal{T}_\text{A}$, which contains all distributions, which solve $\mathbf{G} \mathfrak{p} = \mathbf{G} \mathfrak{f}$:
   ```{r alternativeDistribution}
   pA <- NULL
   if (pH$converged == TRUE){
     pA <- iProjector(G, targets = G %*% entities$empirical, v = pH$p)
     if (pA$converged == TRUE){
       (idiv <- iDivergence(pA$p, pH$p))
     } else{
       (idiv <- Inf)
     }
   } else{
     idiv <- NA
   }
   ```
   We need to check whether we were able to construct the hypothesis distribution first. If not this means that we, formally speaking, cannot compute a $p$-value. Next, we need to check whether we were able to construct the alternative distribution. If not this means that the data cannot be sampled from the hypothesis distribution: the $p$-value is zero.

7. Compute the $p$-value (if applicable)
   ```{r, pValue}
   (p.value <- pchisq(
     2 * attr(entities, "N") * idiv, 
     df = 2, 
     lower.tail = FALSE)
    )

   ```
   Note that we use two degrees of freedom here, because we have two hypothesis conditions!

So the $I$-test reveals that the mean of the ``r what`` attribute differs in at least two Species, which is confirmed by looking at the data:
```{r plotData}
stripchart(formula(paste(what, "~Species")), data = iris, method = "jitter")
```
